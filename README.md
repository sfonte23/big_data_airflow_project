# Big Data Pipeline com Apache Airflow

Este projeto cria um pipeline de dados fake e simula as camadas Raw, Bronze, Silver e Gold utilizando Apache Airflow para orquestraÃ§Ã£o.

---

## Estrutura do Projeto

big_data_airflow_project/  
â”œâ”€â”€ dags/                    # DAGs do Airflow  
â”‚   â”œâ”€â”€ big_data_pipeline_dag.py  
â”‚   â”œâ”€â”€ generate_raw_data_dag.py  
â”‚   â”œâ”€â”€ transform_bronze_dag.py  
â”‚   â”œâ”€â”€ transform_silver_dag.py  
â”‚   â””â”€â”€ transform_gold_dag.py  
â”œâ”€â”€ scripts/                 # Scripts Python para geraÃ§Ã£o e transformaÃ§Ã£o dos dados  
â”‚   â”œâ”€â”€ generate_data.py  
â”‚   â”œâ”€â”€ transform_bronze.py  
â”‚   â”œâ”€â”€ transform_silver.py  
â”‚   â””â”€â”€ transform_gold.py  
â”œâ”€â”€ data/                    # Dados em diferentes camadas (raw, bronze, silver, gold)  
â”œâ”€â”€ docker-compose.yaml      # ConfiguraÃ§Ã£o Docker para Airflow e Postgres  
â””â”€â”€ README.md                # Este arquivo  

---

## ğŸ› ï¸ PrÃ©-requisitos

Certifique-se de ter o seguinte instalado em sua mÃ¡quina:

* **Docker** e **Docker Compose**
* **Python 3.13** (opcional, para rodar scripts localmente)
* Sistema Operacional: **Windows**, **Linux** ou **macOS**

---

## ğŸš€ Como Rodar

Siga os passos abaixo para configurar e executar o projeto:

1.  **Clone o projeto:**
    ```bash
    git clone <seu-repositorio.git>
    cd big_data_airflow_project
    ```

2.  **Suba os containers do Airflow com Docker Compose:**
    ```bash
    docker compose up -d
    ```

3.  **Inicialize o banco de dados do Airflow** (se for a primeira vez):
    ```bash
    docker compose run airflow-webserver airflow db init
    ```

4.  **Crie o usuÃ¡rio administrador** para login na interface do Airflow:
    ```bash
    docker compose run airflow-webserver airflow users create \
      --username airflow \
      --firstname Admin \
      --lastname User \
      --role Admin \
      --email airflow@example.com \
      --password airflow
    ```

5.  **Acesse a interface do Airflow** no navegador:
    [http://localhost:8080](http://localhost:8080)

    **Credenciais padrÃ£o:**
    * **UsuÃ¡rio:** `airflow`
    * **Senha:** `airflow`

6.  **Ative e execute a DAG `big_data_pipeline`** na interface do Airflow para rodar o pipeline completo.

---

## â¡ï¸ DAGs DisponÃ­veis

Este projeto inclui as seguintes DAGs:

* `generate_raw_data` â€” Gera dados fake em formato JSON (camada **Raw**)
* `transform_bronze` â€” Converte JSON para banco SQLite (camada **Bronze**)
* `transform_silver` â€” Realiza limpeza e validaÃ§Ã£o dos dados (camada **Silver**)
* `transform_gold` â€” AgregaÃ§Ã£o e preparaÃ§Ã£o da camada final de dados (camada **Gold**)
* `big_data_pipeline` â€” DAG orquestradora que executa todas as 4 etapas em sequÃªncia.

---

## ğŸ’¡ Estrutura do Pipeline

O fluxo do pipeline Ã© o seguinte:

generate_raw_data -> transform_bronze -> transform_silver -> transform_gold

---

## âœ‰ï¸ Contatos e Suporte

Para dÃºvidas ou sugestÃµes, sinta-se Ã  vontade para entrar em contato:

**SÃ©rgio Medeiros Fonte**
Email: s.fonte@yahoo.com